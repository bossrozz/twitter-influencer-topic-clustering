{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API Extraction Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool will extract tweets from Twitter API V2, it would require an Academic Research API Key/Bearer Token for this to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import sqlite3\n",
    "from IPython.display import clear_output\n",
    "\n",
    "bearer_token = '' # enter twitter API Bearer token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Accepts a json parameter and setups the bearer token for\n",
    "    Twitter headers\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2FullArchiveSearchPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, params):\n",
    "    \"\"\"\n",
    "    Accepts a url string and parameters for the API Request and\n",
    "    returns the response in json format\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def count_tweets(q_params):\n",
    "    \"\"\"\n",
    "    Calls the Twitter V2 API for getting the counts of tweets\n",
    "    and returns the expected tweets.\n",
    "    \"\"\"\n",
    "\n",
    "    count_url = \"https://api.twitter.com/2/tweets/counts/all\"\n",
    "    cnt_params = {'granularity': 'day', **q_params}\n",
    "    del cnt_params['tweet.fields']\n",
    "    del cnt_params['max_results']\n",
    "\n",
    "    total_twts = 0\n",
    "    while True:\n",
    "        cnt_twts = connect_to_endpoint(count_url, cnt_params)\n",
    "        total_twts += cnt_twts['meta']['total_tweet_count']\n",
    "        try:\n",
    "            cnt_params['next_token'] = cnt_twts['meta']['next_token']\n",
    "        except KeyError as e:\n",
    "            if e.args[0] == 'next_token':\n",
    "                break\n",
    "            else:\n",
    "                raise e\n",
    "    return total_twts\n",
    "\n",
    "\n",
    "def get_all_tweets(username, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Calls the TWitter V2 API for searching all the tweets based\n",
    "    on passed username's along with the start and end date. \n",
    "\n",
    "    Returns the list of tweets fetched.\n",
    "    \"\"\"\n",
    "\n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "    query_params = {\n",
    "        'query': f\"(from:{username}) -is:retweet\",\n",
    "        'start_time': start_date,\n",
    "        'end_time': end_date,\n",
    "        'tweet.fields': ('author_id,context_annotations,'\n",
    "                         'conversation_id,created_at,'\n",
    "                         'entities,id,in_reply_to_user_id,lang,'\n",
    "                         'public_metrics,possibly_sensitive,'\n",
    "                         'referenced_tweets,reply_settings,'\n",
    "                         'source,text'),\n",
    "        'max_results': 100\n",
    "    }\n",
    "\n",
    "    total_twts = count_tweets(query_params)\n",
    "    print(f\"GETTING {total_twts} tweets\")\n",
    "\n",
    "    tweets = []\n",
    "\n",
    "    slp_counter = 0\n",
    "    while True:\n",
    "        if slp_counter >= 300:\n",
    "            for i in range(15):\n",
    "                print(f\"Sleeping for {15-i} min\")\n",
    "                time.sleep(61.0)\n",
    "            slp_counter = 0\n",
    "\n",
    "        response = connect_to_endpoint(search_url, query_params)\n",
    "        slp_counter += 1\n",
    "\n",
    "        for tweet in response.get('data'):\n",
    "            tweets.append(tweet)\n",
    "\n",
    "        try:\n",
    "            query_params['next_token'] = response['meta']['next_token']\n",
    "        except KeyError as e:\n",
    "            if e.args[0] == 'next_token':\n",
    "                break\n",
    "            else:\n",
    "                raise e\n",
    "        print(f\"Processed {len(tweets)} out of {total_twts} tweets\")\n",
    "\n",
    "    print(f\"PROCESSED {len(tweets)} tweets\")\n",
    "\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def get_username(usr_id):\n",
    "    \"\"\"\n",
    "    Gets the username of the twitter user based on the user id and returns it\n",
    "    \"\"\"\n",
    "\n",
    "    usr_response = connect_to_endpoint(\n",
    "        f\"https://api.twitter.com/2/users/{usr_id}\", {})\n",
    "    return usr_response['data']['username']\n",
    "\n",
    "\n",
    "def scrape_twitter(username, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Processes tweets fetched for the Twitter API and builds the Data Frame\n",
    "    for the record collection and return it,\n",
    "    \"\"\"\n",
    "\n",
    "    tweets = get_all_tweets(username, start_date, end_date)\n",
    "    df = pd.DataFrame(tweets)\n",
    "\n",
    "    usr_ids = df['author_id'].unique()\n",
    "    usr_lookups = {}\n",
    "    for i, usr in enumerate(usr_ids):\n",
    "        usr_lookups[usr] = get_username(usr)\n",
    "        print(f\"Compiled {i+1} out of {len(usr_ids)} user information\")\n",
    "\n",
    "    df['username'] = df['author_id'].apply(lambda x: usr_lookups[x])\n",
    "\n",
    "    try:\n",
    "        df['url'] = df[['username', 'tweet_id']].apply(lambda x:\n",
    "                                                       f\"twitter.com/{x.username}/status/{x.tweet_id}\", axis=1)\n",
    "    except:\n",
    "        df['url'] = df[['username', 'id']].apply(lambda x:\n",
    "                                                 f\"twitter.com/{x.username}/status/{x.id}\", axis=1)\n",
    "\n",
    "    for i in ['context_annotations', 'public_metrics', 'entities',\n",
    "              'edit_history_tweet_ids', 'referenced_tweets']:\n",
    "        if i in df.columns:\n",
    "            df[i] = df[i].astype('str')\n",
    "        else:\n",
    "            df[i] = ''\n",
    "    print(\"DONE\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets(influencers):\n",
    "    \"\"\"\n",
    "    Accepts a list of twitter user handles without @ i.e. cz_binance.\n",
    "    Each twitter account would be scrape based on the filtered dates \n",
    "    bull and bear market dates assigned. \n",
    "    \n",
    "    Fetched tweets will be saved into twitter_dump table.\n",
    "    \"\"\"\n",
    "    \n",
    "    for inf in influencers:\n",
    "        clear_output(wait=True)\n",
    "        print(f'Getting Tweets for {inf}')\n",
    "        df1 = scrape_twitter(inf, '2021-10-01T00:00:00Z',\n",
    "                             '2021-11-30T23:59:59Z')  \n",
    "        df1.to_sql('twitter_dump', conn, if_exists='append', index=False)\n",
    "        df2 = scrape_twitter(inf, '2022-10-01T00:00:00Z',\n",
    "                             '2022-11-25T23:59:59Z') \n",
    "        df2.to_sql('twitter_dump', conn, if_exists='append', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Tweets for cz_binance\n",
      "GETTING 151 tweets\n",
      "Processed 100 out of 151 tweets\n",
      "PROCESSED 151 tweets\n",
      "Compiled 1 out of 1 user information\n",
      "DONE\n",
      "GETTING 705 tweets\n",
      "Processed 100 out of 705 tweets\n",
      "Processed 200 out of 705 tweets\n",
      "Processed 300 out of 705 tweets\n",
      "Processed 400 out of 705 tweets\n",
      "Processed 493 out of 705 tweets\n",
      "Processed 593 out of 705 tweets\n",
      "Processed 693 out of 705 tweets\n",
      "PROCESSED 698 tweets\n",
      "Compiled 1 out of 1 user information\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "influencers = ['cz_binance']\n",
    "load_tweets(influencers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(*)\n",
       "0     39993"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query(\"SELECT count(*) FROM twitter_dump\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"delete from twitter_dump where username=''\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c48e784580535a0043e835f347c242c3ba8d520cc19d6dd73066c5c113ac8a1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
